{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04ae9bf1-2629-4b0e-b00d-57aa1d4a5548",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch_geometric.data import DataLoader, Dataset\n",
    "from torch_geometric.nn import GCNConv, GraphSAGE, GATConv, GINConv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from torch_geometric.datasets import Planetoid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09f79046-e5bc-421f-b520-e50498284c7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The target and shadow datasets are disjoint.\n",
      "Dataset: PubMed()\n",
      "Number of nodes: 19717\n",
      "Number of edges: 88648\n",
      "Number of features: 500\n",
      "Number of classes: 3\n",
      "Train Mask: 9858 nodes\n",
      "Test Mask: 1000 nodes\n",
      "Target Dataset Nodes: 9858\n",
      "Shadow Dataset Nodes: 9859\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "seed = 42  # You can change this number, but it should be the same across all runs\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "# Load the PubMed dataset\n",
    "dataset = Planetoid(root='/tmp/PubMed', name='PubMed')\n",
    "data = dataset[0]\n",
    "\n",
    "# Split nodes into target and shadow sets\n",
    "nodes = np.arange(data.num_nodes)\n",
    "target_nodes, shadow_nodes = train_test_split(nodes, test_size=0.5, random_state=seed)\n",
    "\n",
    "# Verify disjoint sets\n",
    "overlap = np.intersect1d(target_nodes, shadow_nodes)\n",
    "if len(overlap) == 0:\n",
    "    print(\"The target and shadow datasets are disjoint.\")\n",
    "else:\n",
    "    print(f\"The target and shadow datasets are not disjoint. Overlapping nodes: {len(overlap)}\")\n",
    "\n",
    "\n",
    "# Create training masks\n",
    "data.train_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
    "data.train_mask[target_nodes] = True\n",
    "\n",
    "shadow_data = data.clone()\n",
    "shadow_data.train_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
    "shadow_data.train_mask[shadow_nodes] = True\n",
    "\n",
    "# Inspect dataset\n",
    "print(f\"Dataset: {dataset}\")\n",
    "print(f\"Number of nodes: {data.num_nodes}\")\n",
    "print(f\"Number of edges: {data.num_edges}\")\n",
    "print(f\"Number of features: {data.num_features}\")\n",
    "print(f\"Number of classes: {dataset.num_classes}\")\n",
    "print(f\"Train Mask: {data.train_mask.sum()} nodes\")\n",
    "print(f\"Test Mask: {data.test_mask.sum()} nodes\")\n",
    "print(f\"Target Dataset Nodes: {len(target_nodes)}\")\n",
    "print(f\"Shadow Dataset Nodes: {len(shadow_nodes)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe6f6e78-fa01-4876-99f3-6d2f3c1019f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, out_channels)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.dropout(x, p=0.5, training=self.training)  # 0.5 dropout rate\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return F.log_softmax(x, dim=1)  # Log-softmax for classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec701606-dd43-4e1c-b4e9-76735c47a814",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Target Model...\n",
      "Epoch 50/200, Loss: 0.8198\n",
      "Epoch 100/200, Loss: 0.5715\n",
      "Epoch 150/200, Loss: 0.4616\n",
      "Epoch 200/200, Loss: 0.4118\n",
      "Training Shadow Model...\n",
      "Epoch 50/200, Loss: 0.8217\n",
      "Epoch 100/200, Loss: 0.5711\n",
      "Epoch 150/200, Loss: 0.4567\n",
      "Epoch 200/200, Loss: 0.4043\n",
      "Models trained successfully.\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "hidden_channels = 128\n",
    "learning_rate = 0.001\n",
    "epochs = 200\n",
    "\n",
    "# Initialize models\n",
    "target_model = GCN(dataset.num_node_features, hidden_channels, dataset.num_classes)\n",
    "shadow_model = GCN(dataset.num_node_features, hidden_channels, dataset.num_classes)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "target_optimizer = torch.optim.Adam(target_model.parameters(), lr=learning_rate)\n",
    "shadow_optimizer = torch.optim.Adam(shadow_model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Function to train a model\n",
    "def train_model(model, optimizer, data, epochs):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "        loss = criterion(out[data.train_mask], data.y[data.train_mask])  # Masked training loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if (epoch + 1) % 50 == 0:  # Log progress every 50 epochs\n",
    "            print(f\"Epoch {epoch + 1}/{epochs}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Train the target and shadow models\n",
    "print(\"Training Target Model...\")\n",
    "train_model(target_model, target_optimizer, data, epochs)\n",
    "\n",
    "print(\"Training Shadow Model...\")\n",
    "train_model(shadow_model, shadow_optimizer, shadow_data, epochs)\n",
    "\n",
    "print(\"Models trained successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dbffcf78-38df-429e-8a17-5c369027b01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Baseline0(nn.Module):\n",
    "    def __init__(self, in_features):\n",
    "        super(Baseline0, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_features, 128)\n",
    "        self.fc2 = nn.Linear(128, 32)\n",
    "        self.fc3 = nn.Linear(32, 2)  # Binary classification\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, p=0.5, training=self.training)  # Dropout rate = 0.5\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.fc3(x)\n",
    "        return F.log_softmax(x, dim=1)  # Log-softmax for binary classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "690430f0-f1e6-4fb9-b6df-7ac22852ad3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Baseline1(nn.Module):\n",
    "    def __init__(self, in_features):\n",
    "        super(Baseline1, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_features, 16)\n",
    "        self.fc2 = nn.Linear(16, 2)  # Binary classification\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "28feb31c-497a-4635-891f-1fb72a585670",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Baseline2(nn.Module):\n",
    "    def __init__(self, node_features, graph_features):\n",
    "        super(Baseline2, self).__init__()\n",
    "        # Sub-network for node attributes\n",
    "        self.node_fc1 = nn.Linear(node_features, 256)\n",
    "        self.node_fc2 = nn.Linear(256, 64)\n",
    "        self.node_fc3 = nn.Linear(64, 8)\n",
    "\n",
    "        # Sub-network for graph features\n",
    "        self.graph_fc = nn.Linear(graph_features, 1)\n",
    "\n",
    "        # Final layer\n",
    "        self.final_fc = nn.Linear(8 + 1, 2)  # Concatenated inputs, binary classification\n",
    "\n",
    "    def forward(self, node_x, graph_x):\n",
    "        # Node attributes sub-network\n",
    "        x1 = F.relu(self.node_fc1(node_x))\n",
    "        x1 = F.dropout(x1, p=0.5, training=self.training)\n",
    "        x1 = F.relu(self.node_fc2(x1))\n",
    "        x1 = F.dropout(x1, p=0.5, training=self.training)\n",
    "        x1 = F.relu(self.node_fc3(x1))\n",
    "\n",
    "        # Graph features sub-network\n",
    "        x2 = F.relu(self.graph_fc(graph_x))\n",
    "\n",
    "        # Concatenate and pass through final layer\n",
    "        x = torch.cat([x1, x2], dim=1)\n",
    "        x = self.final_fc(x)\n",
    "        return F.log_softmax(x, dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "136120e8-afa4-43b2-a8c0-d42dd92ae9f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline models, optimizers, and schedulers initialized.\n"
     ]
    }
   ],
   "source": [
    "# Define Baseline Models\n",
    "baseline0 = Baseline0(in_features=data.x.size(1))\n",
    "baseline1 = Baseline1(in_features=data.x.size(1))\n",
    "baseline2 = Baseline2(node_features=data.x.size(1), graph_features=1)\n",
    "\n",
    "# Define loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define optimizers for each baseline model\n",
    "learning_rate = 0.001\n",
    "optimizer_baseline0 = torch.optim.Adam(baseline0.parameters(), lr=learning_rate)\n",
    "optimizer_baseline1 = torch.optim.Adam(baseline1.parameters(), lr=learning_rate)\n",
    "optimizer_baseline2 = torch.optim.Adam(baseline2.parameters(), lr=learning_rate)\n",
    "\n",
    "# Define cosine annealing schedulers\n",
    "epochs = 200\n",
    "scheduler_baseline0 = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer_baseline0, T_max=epochs)\n",
    "scheduler_baseline1 = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer_baseline1, T_max=epochs)\n",
    "scheduler_baseline2 = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer_baseline2, T_max=epochs)\n",
    "\n",
    "print(\"Baseline models, optimizers, and schedulers initialized.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b3b8fd10-c163-4336-ad1e-8565607495e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PosteriorAttackModel(nn.Module):\n",
    "    def __init__(self, in_features):\n",
    "        super(PosteriorAttackModel, self).__init__()\n",
    "        # Three linear layers with sizes 128, 32, and 2 neurons\n",
    "        self.fc1 = nn.Linear(in_features, 128)\n",
    "        self.fc2 = nn.Linear(128, 32)\n",
    "        self.fc3 = nn.Linear(32, 2)  # Output size is 2 for binary classification\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))  # ReLU activation for the first layer\n",
    "        x = F.dropout(x, p=0.5, training=self.training)  # Dropout for regularization\n",
    "        x = F.relu(self.fc2(x))  # ReLU activation for the second layer\n",
    "        x = F.dropout(x, p=0.5, training=self.training)  # Dropout for regularization\n",
    "        x = self.fc3(x)  # Final layer for binary classification\n",
    "        return F.log_softmax(x, dim=1)  # Log-softmax for binary classification output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0b3d3891-db4b-49c7-bb02-4aba331e062b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Posterior-only Attack model, optimizer, and scheduler initialized.\n"
     ]
    }
   ],
   "source": [
    "# Initialize Posterior Attack Model\n",
    "posterior_attack_model = PosteriorAttackModel(in_features=data.x.size(1))  # Adjust input dimension accordingly\n",
    "\n",
    "# Loss function and optimizer for Posterior Attack Model\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "learning_rate = 0.001\n",
    "optimizer_posterior = torch.optim.Adam(posterior_attack_model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Cosine Annealing Scheduler\n",
    "epochs = 200\n",
    "scheduler_posterior = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer_posterior, T_max=epochs)\n",
    "\n",
    "print(\"Posterior-only Attack model, optimizer, and scheduler initialized.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "24544922-7444-4e56-9944-70c5fe66f5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, optimizer, scheduler, data, epochs):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        out = model(data.x)  # Model output with shape [batch_size, num_classes]\n",
    "        \n",
    "        # Ensure that target labels are correctly indexed and in the valid range [0, 1]\n",
    "        target = data.y[data.train_mask]  # Target labels for the current training mask\n",
    "        target = torch.clamp(target, min=0, max=1)  # Clamp to ensure valid binary targets\n",
    "        \n",
    "        # Compute the loss\n",
    "        loss = criterion(out[data.train_mask], target)\n",
    "        \n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        \n",
    "        # Optimizer step\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Scheduler step (for learning rate annealing)\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Print training progress every 10 epochs\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch}/{epochs}, Loss: {loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9600889-b9c9-4baa-92cd-fa282bd3ca37",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
