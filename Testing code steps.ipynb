{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04ae9bf1-2629-4b0e-b00d-57aa1d4a5548",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch_geometric.data import DataLoader, Dataset\n",
    "from torch_geometric.nn import GCNConv, GraphSAGE, GATConv, GINConv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from torch_geometric.datasets import Planetoid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09f79046-e5bc-421f-b520-e50498284c7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.x\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.tx\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.allx\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.y\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.ty\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.ally\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.graph\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.test.index\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: PubMed()\n",
      "Number of nodes: 19717\n",
      "Number of edges: 88648\n",
      "Number of features: 500\n",
      "Number of classes: 3\n",
      "Train Mask: 9858 nodes\n",
      "Test Mask: 1000 nodes\n",
      "Target Dataset Nodes: 9858\n",
      "Shadow Dataset Nodes: 9859\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "seed = 42  # You can change this number, but it should be the same across all runs\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "# Load the PubMed dataset\n",
    "dataset = Planetoid(root='/tmp/PubMed', name='PubMed')\n",
    "data = dataset[0]\n",
    "\n",
    "# Split nodes into target and shadow sets\n",
    "nodes = np.arange(data.num_nodes)\n",
    "target_nodes, shadow_nodes = train_test_split(nodes, test_size=0.5, random_state=seed)\n",
    "target_train_nodes, target_test_nodes = train_test_split(target_nodes, test_size=0.2, random_state=seed)\n",
    "\n",
    "# # Verify disjoint sets\n",
    "# overlap = np.intersect1d(target_nodes, shadow_nodes)\n",
    "# if len(overlap) == 0:\n",
    "#     print(\"The target and shadow datasets are disjoint.\")\n",
    "# else:\n",
    "#     print(f\"The target and shadow datasets are not disjoint. Overlapping nodes: {len(overlap)}\")\n",
    "\n",
    "\n",
    "# Create training masks\n",
    "data.train_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
    "data.train_mask[target_nodes] = True\n",
    "\n",
    "shadow_data = data.clone()\n",
    "shadow_data.train_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
    "shadow_data.train_mask[shadow_nodes] = True\n",
    "\n",
    "# Inspect dataset\n",
    "print(f\"Dataset: {dataset}\")\n",
    "print(f\"Number of nodes: {data.num_nodes}\")\n",
    "print(f\"Number of edges: {data.num_edges}\")\n",
    "print(f\"Number of features: {data.num_features}\")\n",
    "print(f\"Number of classes: {dataset.num_classes}\")\n",
    "print(f\"Train Mask: {data.train_mask.sum()} nodes\")\n",
    "print(f\"Test Mask: {data.test_mask.sum()} nodes\")\n",
    "print(f\"Target Dataset Nodes: {len(target_nodes)}\")\n",
    "print(f\"Shadow Dataset Nodes: {len(shadow_nodes)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe6f6e78-fa01-4876-99f3-6d2f3c1019f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, out_channels)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.dropout(x, p=0.5, training=self.training)  # 0.5 dropout rate\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return F.log_softmax(x, dim=1)  # Log-softmax for classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec701606-dd43-4e1c-b4e9-76735c47a814",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Target Model...\n",
      "Epoch 50/200, Loss: 0.8289\n",
      "Epoch 100/200, Loss: 0.5806\n",
      "Epoch 150/200, Loss: 0.4636\n",
      "Epoch 200/200, Loss: 0.4121\n",
      "Training Shadow Model...\n",
      "Epoch 50/200, Loss: 0.8350\n",
      "Epoch 100/200, Loss: 0.5727\n",
      "Epoch 150/200, Loss: 0.4567\n",
      "Epoch 200/200, Loss: 0.4047\n",
      "Models trained successfully.\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "hidden_channels = 128\n",
    "learning_rate = 0.001\n",
    "epochs = 200\n",
    "\n",
    "# Initialize models\n",
    "target_model = GCN(dataset.num_node_features, hidden_channels, dataset.num_classes)\n",
    "shadow_model = GCN(dataset.num_node_features, hidden_channels, dataset.num_classes)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "target_optimizer = torch.optim.Adam(target_model.parameters(), lr=learning_rate)\n",
    "shadow_optimizer = torch.optim.Adam(shadow_model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Function to train a model\n",
    "def train_model(model, optimizer, data, epochs):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "        loss = criterion(out[data.train_mask], data.y[data.train_mask])  # Masked training loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if (epoch + 1) % 50 == 0:  # Log progress every 50 epochs\n",
    "            print(f\"Epoch {epoch + 1}/{epochs}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Train the target and shadow models\n",
    "print(\"Training Target Model...\")\n",
    "train_model(target_model, target_optimizer, data, epochs)\n",
    "\n",
    "print(\"Training Shadow Model...\")\n",
    "train_model(shadow_model, shadow_optimizer, shadow_data, epochs)\n",
    "\n",
    "print(\"Models trained successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62d2fba1-d488-47da-8446-7326b0f4619a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8640\n",
      "Accuracy: 0.8650\n"
     ]
    }
   ],
   "source": [
    "def evaluate_attack(model, data):\n",
    "    model.eval()\n",
    "    out = model(data)\n",
    "    pred = out.argmax(dim=1)\n",
    "    correct = (pred[data.test_mask] == data.y[data.test_mask]).sum().item()\n",
    "    accuracy = correct / data.test_mask.sum().item()\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    # AUC calculation would require a link prediction setup, which is not included here\n",
    "\n",
    "evaluate_attack(target_model, data)\n",
    "evaluate_attack(shadow_model, shadow_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6ee5918-7767-4e71-aef2-b53259f2d3d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target Model AUC: 0.8594\n",
      "Shadow Model AUC: 0.8744\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "import torch\n",
    "\n",
    "# Function to predict links (links that are predicted to be connected)\n",
    "def predict_links(model, data, positive_pairs, negative_pairs):\n",
    "    model.eval()\n",
    "    pred_scores = []\n",
    "    \n",
    "    # Get predictions for positive and negative pairs\n",
    "    for pair in positive_pairs + negative_pairs:\n",
    "        node1, node2 = pair\n",
    "        edge_index = torch.tensor([[node1, node2], [node2, node1]], dtype=torch.long).t()\n",
    "        out = model(data)\n",
    "        score = out[node1].dot(out[node2])  # Cosine similarity or dot product\n",
    "        pred_scores.append(score.item())\n",
    "    \n",
    "    return pred_scores\n",
    "\n",
    "# Generate positive and negative pairs\n",
    "def generate_link_pairs(data, num_pairs=1000):\n",
    "    # Positive pairs (edges)\n",
    "    positive_pairs = [(u, v) for u, v in zip(*data.edge_index)]\n",
    "    \n",
    "    # Negative pairs (non-edges)\n",
    "    negative_pairs = []\n",
    "    while len(negative_pairs) < num_pairs:\n",
    "        node1, node2 = torch.randint(0, data.num_nodes, (2,))\n",
    "        if not torch.any((data.edge_index[0] == node1) & (data.edge_index[1] == node2)):\n",
    "            negative_pairs.append((node1.item(), node2.item()))\n",
    "    \n",
    "    return positive_pairs[:num_pairs], negative_pairs\n",
    "\n",
    "# AUC Calculation\n",
    "def calculate_auc(pred_scores, positive_labels):\n",
    "    return roc_auc_score(positive_labels, pred_scores)\n",
    "\n",
    "# Example usage:\n",
    "positive_pairs, negative_pairs = generate_link_pairs(data)\n",
    "positive_labels = [1] * len(positive_pairs) + [0] * len(negative_pairs)\n",
    "\n",
    "# Get prediction scores for target model\n",
    "target_pred_scores = predict_links(target_model, data, positive_pairs, negative_pairs)\n",
    "\n",
    "# Calculate AUC for the target model\n",
    "target_auc = calculate_auc(target_pred_scores, positive_labels)\n",
    "print(f\"Target Model AUC: {target_auc:.4f}\")\n",
    "\n",
    "# Get prediction scores for shadow model\n",
    "shadow_pred_scores = predict_links(shadow_model, shadow_data, positive_pairs, negative_pairs)\n",
    "\n",
    "# Calculate AUC for the shadow model\n",
    "shadow_auc = calculate_auc(shadow_pred_scores, positive_labels)\n",
    "print(f\"Shadow Model AUC: {shadow_auc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dbffcf78-38df-429e-8a17-5c369027b01e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stolen Links (sorted by prediction score): [(tensor(11894), tensor(62), 270.33013916015625), (tensor(11450), tensor(62), 248.64093017578125), (tensor(1205), tensor(60), 234.48582458496094), (tensor(11894), tensor(88), 204.99546813964844), (tensor(13940), tensor(117), 188.9935302734375)]\n"
     ]
    }
   ],
   "source": [
    "# Link Stealing Attack: Use shadow model to predict links for the target graph\n",
    "def link_stealing_attack(shadow_model, target_data):\n",
    "    shadow_model.eval()\n",
    "    stolen_links = []\n",
    "    \n",
    "    # Use shadow model to predict links on the target data\n",
    "    for pair in generate_link_pairs(target_data)[0]:  # Get positive pairs from target graph\n",
    "        node1, node2 = pair\n",
    "        score = predict_links(shadow_model, target_data, [pair], [])[0]\n",
    "        stolen_links.append((node1, node2, score))\n",
    "        \n",
    "        # # Sort stolen links by score in descending order\n",
    "        # stolen_links = sorted(stolen_links, key=lambda x: x[2], reverse=True)\n",
    "        # return stolen_links\n",
    "    \n",
    "    # Sort stolen links by score\n",
    "    stolen_links.sort(key=lambda x: x[2], reverse=True)\n",
    "    return stolen_links\n",
    "\n",
    "# Evaluate Link Stealing Attack\n",
    "stolen_links = link_stealing_attack(shadow_model, data)\n",
    "print(\"Stolen Links (sorted by prediction score):\", stolen_links[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e58741dc-e86a-4cf3-8453-e73de2248651",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 Stolen Links:\n",
      "Link 1: Node 11894 ↔ Node 62, Score: 270.3301\n",
      "Link 2: Node 11450 ↔ Node 62, Score: 248.6409\n",
      "Link 3: Node 1205 ↔ Node 60, Score: 234.4858\n",
      "Link 4: Node 11894 ↔ Node 88, Score: 204.9955\n",
      "Link 5: Node 13940 ↔ Node 117, Score: 188.9935\n",
      "Link 6: Node 4058 ↔ Node 88, Score: 164.6518\n",
      "Link 7: Node 12019 ↔ Node 105, Score: 161.6323\n",
      "Link 8: Node 11894 ↔ Node 147, Score: 152.0048\n",
      "Link 9: Node 8106 ↔ Node 117, Score: 141.9617\n",
      "Link 10: Node 9046 ↔ Node 117, Score: 135.9112\n"
     ]
    }
   ],
   "source": [
    "print(\"Top 10 Stolen Links:\") \n",
    "for i, link in enumerate(stolen_links[:10]): print(f\"Link {i+1}: Node {link[0]} ↔ Node {link[1]}, Score: {link[2]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ff25899-f823-4d07-afa2-23a7bc618b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_attack_success(stolen_links, actual_positive_pairs): \n",
    "    matched_links = [(u, v) for u, v, _ in stolen_links if (u, v) in actual_positive_pairs] \n",
    "    attack_success_rate = len(matched_links) / len(actual_positive_pairs) \n",
    "    print(f\"Attack Success Rate: {attack_success_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "62c3f75f-2dc6-486a-8308-ab92dda998ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_positive_pairs = [(u, v) for u, v in zip(*data.edge_index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fc894643-2720-4d4a-8841-cd29c294aa99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attack Success Rate: 0.0113\n"
     ]
    }
   ],
   "source": [
    "evaluate_attack_success(stolen_links, actual_positive_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7e5e20b4-056f-4efe-96ca-562a4f8a2219",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Baseline0(nn.Module):\n",
    "    def __init__(self, in_features):\n",
    "        super(Baseline0, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_features, 128)\n",
    "        self.fc2 = nn.Linear(128, 32)\n",
    "        self.fc3 = nn.Linear(32, 2)  # Binary classification\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, p=0.5, training=self.training)  # Dropout rate = 0.5\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.fc3(x)\n",
    "        return F.log_softmax(x, dim=1)  # Log-softmax for binary classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "690430f0-f1e6-4fb9-b6df-7ac22852ad3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Baseline1(nn.Module):\n",
    "    def __init__(self, in_features):\n",
    "        super(Baseline1, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_features, 16)\n",
    "        self.fc2 = nn.Linear(16, 2)  # Binary classification\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "28feb31c-497a-4635-891f-1fb72a585670",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Baseline2(nn.Module):\n",
    "    def __init__(self, node_features, graph_features):\n",
    "        super(Baseline2, self).__init__()\n",
    "        # Sub-network for node attributes\n",
    "        self.node_fc1 = nn.Linear(node_features, 256)\n",
    "        self.node_fc2 = nn.Linear(256, 64)\n",
    "        self.node_fc3 = nn.Linear(64, 8)\n",
    "\n",
    "        # Sub-network for graph features\n",
    "        self.graph_fc = nn.Linear(graph_features, 1)\n",
    "\n",
    "        # Final layer\n",
    "        self.final_fc = nn.Linear(8 + 1, 2)  # Concatenated inputs, binary classification\n",
    "\n",
    "    def forward(self, node_x, graph_x):\n",
    "        # Node attributes sub-network\n",
    "        x1 = F.relu(self.node_fc1(node_x))\n",
    "        x1 = F.dropout(x1, p=0.5, training=self.training)\n",
    "        x1 = F.relu(self.node_fc2(x1))\n",
    "        x1 = F.dropout(x1, p=0.5, training=self.training)\n",
    "        x1 = F.relu(self.node_fc3(x1))\n",
    "\n",
    "        # Graph features sub-network\n",
    "        x2 = F.relu(self.graph_fc(graph_x))\n",
    "\n",
    "        # Concatenate and pass through final layer\n",
    "        x = torch.cat([x1, x2], dim=1)\n",
    "        x = self.final_fc(x)\n",
    "        return F.log_softmax(x, dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "136120e8-afa4-43b2-a8c0-d42dd92ae9f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline models, optimizers, and schedulers initialized.\n",
      "<torch.optim.lr_scheduler.CosineAnnealingLR object at 0x153ee5220>\n"
     ]
    }
   ],
   "source": [
    "# Define Baseline Models\n",
    "baseline0 = Baseline0(in_features=data.x.size(1))\n",
    "baseline1 = Baseline1(in_features=data.x.size(1))\n",
    "baseline2 = Baseline2(node_features=data.x.size(1), graph_features=1)\n",
    "\n",
    "\n",
    "# Define loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define optimizers for each baseline model\n",
    "learning_rate = 0.001\n",
    "optimizer_baseline0 = torch.optim.Adam(baseline0.parameters(), lr=learning_rate)\n",
    "optimizer_baseline1 = torch.optim.Adam(baseline1.parameters(), lr=learning_rate)\n",
    "optimizer_baseline2 = torch.optim.Adam(baseline2.parameters(), lr=learning_rate)\n",
    "\n",
    "# Define cosine annealing schedulers\n",
    "epochs = 200\n",
    "scheduler_baseline0 = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer_baseline0, T_max=epochs)\n",
    "scheduler_baseline1 = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer_baseline1, T_max=epochs)\n",
    "scheduler_baseline2 = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer_baseline2, T_max=epochs)\n",
    "\n",
    "print(\"Baseline models, optimizers, and schedulers initialized.\")\n",
    "print(scheduler_baseline0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79b61dc-a7a7-4064-885b-5fc991972a69",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
